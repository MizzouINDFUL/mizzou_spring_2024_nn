{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fun with optimization\n",
    "\n",
    "Lets play with SGD, momentum, Adagrad, and Adam. Lets do this for a simple 2D problem that we can visualize! Yes, I know, I could improve the initilization of the algorithms, the efficiency of my codes, yes my code might crash because I did not defensively program them, etc. The point of this Jupyter page is simply to give us a little optimization sand box to play with. \n",
    "\n",
    "First, declare some lib's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets define our function to optimize and then calc its derivative\n",
    "\n",
    "I put three functions in there for you so you can pick and explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what function do you want to run?\n",
    "WhichFx = 3\n",
    "\n",
    "if(WhichFx==1):\n",
    "    \n",
    "    def func_z(x, y):\n",
    "        z = x**2/10. + x*y/50. + y**2.\n",
    "        return z\n",
    "\n",
    "    def der_x(x, y):\n",
    "        return (2*x/10. + y/50.)\n",
    "\n",
    "    def der_y(x, y):\n",
    "        return (x/50. + 2*y/1.)\n",
    "    \n",
    "    bounds = np.asarray([-3,3])\n",
    "    \n",
    "elif(WhichFx==2):\n",
    "    \n",
    "    def func_z(x, y):\n",
    "        z = x**2 + 3 * np.sin(y)\n",
    "        return z\n",
    "\n",
    "    def der_x(x, y):\n",
    "        return 2*x\n",
    "\n",
    "    def der_y(x, y):\n",
    "        return 3 * np.cos(y)    \n",
    "    \n",
    "    bounds = np.asarray([-6,6])\n",
    "    \n",
    "if(WhichFx==3):\n",
    "    \n",
    "    def func_z(x, y):\n",
    "        z = x**4 - 2*x**2 + y**2\n",
    "        return z\n",
    "\n",
    "    def der_x(x, y):\n",
    "        return 4*x**3 - 4*x\n",
    "\n",
    "    def der_y(x, y):\n",
    "        return 2*y\n",
    "    \n",
    "    bounds = np.asarray([-3,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets make a gradient descent function with just a learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descentGD(previous_x, previous_y, learning_rate, epoch):\n",
    "    \n",
    "    # store our gradients\n",
    "    x_gd = []\n",
    "    y_gd = []\n",
    "    z_gd = []\n",
    "\n",
    "    # append the first sampled point\n",
    "    x_gd.append(previous_x)\n",
    "    y_gd.append(previous_y)\n",
    "    z_gd.append(func_z(previous_x, previous_y))\n",
    "\n",
    "    # gradient descent algorithm\n",
    "    for i in range(epoch):\n",
    "\n",
    "        # what is our derivative?\n",
    "        dx = der_x(previous_x,previous_y)\n",
    "        dy = der_y(previous_x,previous_y)\n",
    "        \n",
    "        # simple update in x and y\n",
    "        current_x = previous_x - learning_rate*dx\n",
    "        x_gd.append(current_x)\n",
    "        \n",
    "        current_y = previous_y - learning_rate*dy\n",
    "        y_gd.append(current_y)\n",
    "\n",
    "        z_gd.append(func_z(current_x, current_y))\n",
    "\n",
    "        # update previous_x and previous_y\n",
    "        previous_x = current_x\n",
    "        previous_y = current_y\n",
    "\n",
    "    return x_gd, y_gd, z_gd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets pick some init values\n",
    "\n",
    "I just manually selected these fyi\n",
    "\n",
    "To show you the behavior of these methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(WhichFx==1):\n",
    "    # location to start at \n",
    "    x0 = 3\n",
    "    y0 = 2\n",
    "    # learning rate\n",
    "    learning_rate = 0.9\n",
    "    # number of epochs\n",
    "    epoch = 100\n",
    "elif(WhichFx==2):\n",
    "    x0 = 4\n",
    "    y0 = 1\n",
    "    learning_rate = 0.2\n",
    "    epoch = 100\n",
    "elif(WhichFx==3):\n",
    "    x0 = -2\n",
    "    y0 = -2\n",
    "    learning_rate = 0.15\n",
    "    epoch = 50   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run our algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gd1, y_gd1, z_gd1 = gradient_descentGD(x0, y0, learning_rate, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(bounds[0], bounds[1], 0.05)\n",
    "b = np.arange(bounds[0], bounds[1], 0.05)\n",
    "x, y = np.meshgrid(a, b)\n",
    "z = func_z(x, y)\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.contour(x, y, z, levels=np.logspace(-3, 3, 25), cmap='jet')\n",
    "\n",
    "# Plot our steps\n",
    "ax1.plot(x_gd1, y_gd1, 'ro')\n",
    "for i in range(1, epoch+1):\n",
    "    ax1.annotate('', xy=(x_gd1[i], y_gd1[i]), xytext=(x_gd1[i-1], y_gd1[i-1]),\n",
    "                   arrowprops={'arrowstyle': '->', 'color': 'r', 'lw': 1},\n",
    "                   va='center', ha='center')\n",
    "\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "legend_elements = [Line2D([0], [0], color='r', lw=4, label='GD')]\n",
    "ax1.legend(handles=legend_elements, loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we considered momentum?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descentMOM(previous_x, previous_y, learning_rate, epoch, mom):\n",
    "    \n",
    "    x_gd = []\n",
    "    y_gd = []\n",
    "    z_gd = []\n",
    "\n",
    "    x_v = 0\n",
    "    y_v = 0\n",
    "    \n",
    "    x_gd.append(previous_x)\n",
    "    y_gd.append(previous_y)\n",
    "    z_gd.append(func_z(previous_x, previous_y))\n",
    "\n",
    "    for i in range(epoch):\n",
    "        \n",
    "        dx = der_x(previous_x,previous_y)\n",
    "        dy = der_y(previous_x,previous_y)\n",
    "        \n",
    "        # note, our update changes a tad\n",
    "        x_v = mom * x_v - learning_rate*dx # velocity\n",
    "        current_x = previous_x + x_v # position\n",
    "        x_gd.append(current_x)\n",
    "                \n",
    "        y_v = mom * y_v - learning_rate*dy # velocity\n",
    "        current_y = previous_y + y_v # position\n",
    "        y_gd.append(current_y)\n",
    "\n",
    "        z_gd.append(func_z(current_x, current_y))\n",
    "\n",
    "        previous_x = current_x\n",
    "        previous_y = current_y\n",
    "\n",
    "    return x_gd, y_gd, z_gd\n",
    "\n",
    "mom = 0.25 # what momentum to use?\n",
    "x_gd2, y_gd2, z_gd2 = gradient_descentMOM(x0, y0, learning_rate, epoch, mom)\n",
    "\n",
    "a = np.arange(bounds[0], bounds[1], 0.05)\n",
    "b = np.arange(bounds[0], bounds[1], 0.05)\n",
    "x, y = np.meshgrid(a, b)\n",
    "z = func_z(x, y)\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.contour(x, y, z, levels=np.logspace(bounds[0], bounds[1], 25), cmap='jet')\n",
    "\n",
    "# Plot our steps\n",
    "ax1.plot(x_gd2, y_gd2, 'co')\n",
    "for i in range(1, epoch+1):\n",
    "    ax1.annotate('', xy=(x_gd2[i], y_gd2[i]), xytext=(x_gd2[i-1], y_gd2[i-1]),\n",
    "                   arrowprops={'arrowstyle': '->', 'color': 'c', 'lw': 1},\n",
    "                   va='center', ha='center')\n",
    "\n",
    "# Plot our steps\n",
    "ax1.plot(x_gd1, y_gd1, 'ro')\n",
    "for i in range(1, epoch+1):\n",
    "    ax1.annotate('', xy=(x_gd1[i], y_gd1[i]), xytext=(x_gd1[i-1], y_gd1[i-1]),\n",
    "                   arrowprops={'arrowstyle': '->', 'color': 'r', 'lw': 1},\n",
    "                   va='center', ha='center')\n",
    "    \n",
    "legend_elements = [Line2D([0], [0], color='r', lw=4, label='GD'),\n",
    "                  Line2D([0], [0], color='c', lw=4, label='MOM')]\n",
    "ax1.legend(handles=legend_elements, loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descentAdaGrad(previous_x, previous_y, learning_rate, epoch):\n",
    "    \n",
    "    eps = 1e-8\n",
    "    \n",
    "    x_gd = []\n",
    "    y_gd = []\n",
    "    z_gd = []\n",
    "    \n",
    "    x_gd.append(previous_x)\n",
    "    y_gd.append(previous_y)\n",
    "    z_gd.append(func_z(previous_x, previous_y))\n",
    "\n",
    "    cache_x = cache_y = 0\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        \n",
    "        dx = der_x(previous_x,previous_y)\n",
    "        dy = der_y(previous_x,previous_y)\n",
    "        \n",
    "        cache_x = cache_x + dx**2 \n",
    "        cache_y = cache_y + dy**2\n",
    "        current_x = previous_x - learning_rate * dx / (np.sqrt(cache_x) + eps) \n",
    "        current_y = previous_y - learning_rate * dy / (np.sqrt(cache_y) + eps) \n",
    "\n",
    "        x_gd.append(current_x)\n",
    "        y_gd.append(current_y)\n",
    "            \n",
    "        z_gd.append(func_z(current_x, current_y))\n",
    "\n",
    "        previous_x = current_x\n",
    "        previous_y = current_y\n",
    "\n",
    "    return x_gd, y_gd, z_gd\n",
    "\n",
    "x_gd5, y_gd5, z_gd5 = gradient_descentAdaGrad(x0, y0, learning_rate, epoch)\n",
    "\n",
    "a = np.arange(bounds[0], bounds[1], 0.05)\n",
    "b = np.arange(bounds[0], bounds[1], 0.05)\n",
    "x, y = np.meshgrid(a, b)\n",
    "z = func_z(x, y)\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.contour(x, y, z, levels=np.logspace(bounds[0], bounds[1], 25), cmap='jet')\n",
    "\n",
    "# Plot our steps\n",
    "ax1.plot(x_gd5, y_gd5, 'go')\n",
    "for i in range(1, epoch+1):\n",
    "    ax1.annotate('', xy=(x_gd5[i], y_gd5[i]), xytext=(x_gd5[i-1], y_gd5[i-1]),\n",
    "                   arrowprops={'arrowstyle': '->', 'color': 'g', 'lw': 1},\n",
    "                   va='center', ha='center')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot all on the same screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(bounds[0], bounds[1], 0.05)\n",
    "b = np.arange(bounds[0], bounds[1], 0.05)\n",
    "x, y = np.meshgrid(a, b)\n",
    "z = func_z(x, y)\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.contour(x, y, z, levels=np.logspace(bounds[0], bounds[1], 25), cmap='jet')\n",
    "\n",
    "# Plot our steps\n",
    "ax1.plot(x_gd5, y_gd5, 'go')\n",
    "for i in range(1, epoch+1):\n",
    "    ax1.annotate('', xy=(x_gd5[i], y_gd5[i]), xytext=(x_gd5[i-1], y_gd5[i-1]),\n",
    "                   arrowprops={'arrowstyle': '->', 'color': 'g', 'lw': 1},\n",
    "                   va='center', ha='center')\n",
    "    \n",
    "# Plot our steps\n",
    "ax1.plot(x_gd1, y_gd1, 'ro')\n",
    "for i in range(1, epoch+1):\n",
    "    ax1.annotate('', xy=(x_gd1[i], y_gd1[i]), xytext=(x_gd1[i-1], y_gd1[i-1]),\n",
    "                   arrowprops={'arrowstyle': '->', 'color': 'r', 'lw': 1},\n",
    "                   va='center', ha='center')\n",
    "\n",
    "# Plot our steps\n",
    "ax1.plot(x_gd2, y_gd2, 'co')\n",
    "for i in range(1, epoch+1):\n",
    "    ax1.annotate('', xy=(x_gd2[i], y_gd2[i]), xytext=(x_gd2[i-1], y_gd2[i-1]),\n",
    "                   arrowprops={'arrowstyle': '->', 'color': 'c', 'lw': 1},\n",
    "                   va='center', ha='center')\n",
    "    \n",
    "legend_elements = [Line2D([0], [0], color='r', lw=4, label='GD'),\n",
    "                  Line2D([0], [0], color='c', lw=4, label='MOM'),\n",
    "                  Line2D([0], [0], color='g', lw=4, label='AGrad')]\n",
    "ax1.legend(handles=legend_elements, loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descentRMSProp(previous_x, previous_y, learning_rate, epoch, decay_rate):\n",
    "    \n",
    "    eps = 1e-8\n",
    "    \n",
    "    x_gd = []\n",
    "    y_gd = []\n",
    "    z_gd = []\n",
    "\n",
    "    cache_x = 0\n",
    "    cache_y = 0\n",
    "    \n",
    "    x_gd.append(previous_x)\n",
    "    y_gd.append(previous_y)\n",
    "    z_gd.append(func_z(previous_x, previous_y))\n",
    "\n",
    "    for i in range(epoch):\n",
    "        \n",
    "        dx = der_x(previous_x,previous_y)\n",
    "        dy = der_y(previous_x,previous_y)\n",
    "        \n",
    "        if(i==0):\n",
    "            cache_x = dx**2        \n",
    "        else:\n",
    "            cache_x = decay_rate * cache_x + (1 - decay_rate)*dx**2        \n",
    "        current_x = previous_x - learning_rate*dx / (np.sqrt(cache_x) + eps) \n",
    "        x_gd.append(current_x)\n",
    "        \n",
    "        if(i==0):\n",
    "            cache_y = dy**2         \n",
    "        else:\n",
    "            cache_y = decay_rate * cache_y + (1 - decay_rate)*dy**2         \n",
    "        current_y = previous_y - learning_rate*dy / (np.sqrt(cache_y) + eps) \n",
    "        y_gd.append(current_y)\n",
    "\n",
    "        z_gd.append(func_z(current_x, current_y))\n",
    "\n",
    "        previous_x = current_x\n",
    "        previous_y = current_y\n",
    "\n",
    "    return x_gd, y_gd, z_gd\n",
    "\n",
    "x_gd6, y_gd6, z_gd6 = gradient_descentRMSProp(x0, y0, learning_rate, epoch, decay_rate=0.99)\n",
    "\n",
    "a = np.arange(bounds[0], bounds[1], 0.05)\n",
    "b = np.arange(bounds[0], bounds[1], 0.05)\n",
    "x, y = np.meshgrid(a, b)\n",
    "z = func_z(x, y)\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.contour(x, y, z, levels=np.logspace(bounds[0], bounds[1], 25), cmap='jet')\n",
    "\n",
    "# Plot our steps\n",
    "ax1.plot(x_gd6, y_gd6, 'mo')\n",
    "for i in range(1, epoch+1):\n",
    "    ax1.annotate('', xy=(x_gd6[i], y_gd6[i]), xytext=(x_gd6[i-1], y_gd6[i-1]),\n",
    "                   arrowprops={'arrowstyle': '->', 'color': 'm', 'lw': 1},\n",
    "                   va='center', ha='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot all on the same graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(bounds[0], bounds[1], 0.05)\n",
    "b = np.arange(bounds[0], bounds[1], 0.05)\n",
    "x, y = np.meshgrid(a, b)\n",
    "z = func_z(x, y)\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.contour(x, y, z, levels=np.logspace(bounds[0], bounds[1], 25), cmap='jet')\n",
    "\n",
    "# Plot our steps\n",
    "ax1.plot(x_gd6, y_gd6, 'mo')\n",
    "for i in range(1, epoch+1):\n",
    "    ax1.annotate('', xy=(x_gd6[i], y_gd6[i]), xytext=(x_gd6[i-1], y_gd6[i-1]),\n",
    "                   arrowprops={'arrowstyle': '->', 'color': 'm', 'lw': 1},\n",
    "                   va='center', ha='center')\n",
    "\n",
    "# Plot our steps\n",
    "ax1.plot(x_gd1, y_gd1, 'ro')\n",
    "for i in range(1, epoch+1):\n",
    "    ax1.annotate('', xy=(x_gd1[i], y_gd1[i]), xytext=(x_gd1[i-1], y_gd1[i-1]),\n",
    "                   arrowprops={'arrowstyle': '->', 'color': 'r', 'lw': 1},\n",
    "                   va='center', ha='center')\n",
    "    \n",
    "# Plot our steps\n",
    "ax1.plot(x_gd5, y_gd5, 'go')\n",
    "for i in range(1, epoch+1):\n",
    "    ax1.annotate('', xy=(x_gd5[i], y_gd5[i]), xytext=(x_gd5[i-1], y_gd5[i-1]),\n",
    "                   arrowprops={'arrowstyle': '->', 'color': 'g', 'lw': 1},\n",
    "                   va='center', ha='center')\n",
    "\n",
    "# Plot our steps\n",
    "ax1.plot(x_gd2, y_gd2, 'co')\n",
    "for i in range(1, epoch+1):\n",
    "    ax1.annotate('', xy=(x_gd2[i], y_gd2[i]), xytext=(x_gd2[i-1], y_gd2[i-1]),\n",
    "                   arrowprops={'arrowstyle': '->', 'color': 'c', 'lw': 1},\n",
    "                   va='center', ha='center')   \n",
    "    \n",
    "legend_elements = [Line2D([0], [0], color='r', lw=4, label='GD'),\n",
    "                  Line2D([0], [0], color='c', lw=4, label='MOM'),\n",
    "                  Line2D([0], [0], color='g', lw=4, label='AGrad'),\n",
    "                  Line2D([0], [0], color='m', lw=4, label='RMSProp')]\n",
    "ax1.legend(handles=legend_elements, loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descentAdam(previous_x, previous_y, learning_rate, epoch, beta1, beta2):\n",
    "    \n",
    "    eps = 1e-8\n",
    "    \n",
    "    x_gd = []\n",
    "    y_gd = []\n",
    "    z_gd = []\n",
    "    \n",
    "    x_gd.append(previous_x)\n",
    "    y_gd.append(previous_y)\n",
    "    z_gd.append(func_z(previous_x, previous_y))\n",
    "\n",
    "    mx = mxt = my = myt = vx = vxt = vy = vyt = 0\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        \n",
    "        dx = der_x(previous_x,previous_y)\n",
    "        dy = der_y(previous_x,previous_y)\n",
    "        \n",
    "        if(i==0):\n",
    "            mx = mxt = dx\n",
    "        else:\n",
    "            mx = beta1*mx + (1-beta1)*dx\n",
    "            mxt = mx / (1-beta1**(i+1))\n",
    "        if(i==0):\n",
    "            vx = vxt = dx**2\n",
    "        else:\n",
    "            vx = beta2*vx + (1-beta2)*(dx**2)\n",
    "            vxt = vx / (1-beta2**(i+1))\n",
    "        current_x = previous_x - learning_rate * mxt / (np.sqrt(vxt) + eps)\n",
    "        x_gd.append(current_x)\n",
    "        \n",
    "        if(i==0):\n",
    "            my = myt = dy\n",
    "        else:\n",
    "            my = beta1*my + (1-beta1)*dy\n",
    "            myt = my / (1-beta1**(i+1))\n",
    "        if(i==0):\n",
    "            vy = vyt = dy**2\n",
    "        else:\n",
    "            vy = beta2*vy + (1-beta2)*(dy**2)\n",
    "            vyt = vy / (1-beta2**(i+1))\n",
    "        current_y = previous_y - learning_rate * myt / (np.sqrt(vyt) + eps)                \n",
    "        y_gd.append(current_y)\n",
    "\n",
    "        z_gd.append(func_z(current_x, current_y))\n",
    "\n",
    "        previous_x = current_x\n",
    "        previous_y = current_y\n",
    "\n",
    "    return x_gd, y_gd, z_gd\n",
    "\n",
    "# pick our parameters\n",
    "beta1 = 0.2\n",
    "beta2 = 0.95\n",
    "x_gd7, y_gd7, z_gd7 = gradient_descentAdam(x0, y0, learning_rate, epoch, beta1, beta2)\n",
    "\n",
    "a = np.arange(bounds[0], bounds[1], 0.05)\n",
    "b = np.arange(bounds[0], bounds[1], 0.05)\n",
    "x, y = np.meshgrid(a, b)\n",
    "z = func_z(x, y)\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.contour(x, y, z, levels=np.logspace(bounds[0], bounds[1], 25), cmap='jet')\n",
    "\n",
    "# Plot our steps\n",
    "ax1.plot(x_gd7, y_gd7, 'ko')\n",
    "for i in range(1, epoch+1):\n",
    "    ax1.annotate('', xy=(x_gd7[i], y_gd7[i]), xytext=(x_gd7[i-1], y_gd7[i-1]),\n",
    "                   arrowprops={'arrowstyle': '->', 'color': 'k', 'lw': 1},\n",
    "                   va='center', ha='center')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(bounds[0], bounds[1], 0.05)\n",
    "b = np.arange(bounds[0], bounds[1], 0.05)\n",
    "x, y = np.meshgrid(a, b)\n",
    "z = func_z(x, y)\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.contour(x, y, z, levels=np.logspace(bounds[0], bounds[1], 25), cmap='jet')\n",
    "\n",
    "# Plot our steps\n",
    "ax1.plot(x_gd7, y_gd7, 'ko')\n",
    "for i in range(1, epoch+1):\n",
    "    ax1.annotate('', xy=(x_gd7[i], y_gd7[i]), xytext=(x_gd7[i-1], y_gd7[i-1]),\n",
    "                   arrowprops={'arrowstyle': '->', 'color': 'k', 'lw': 1},\n",
    "                   va='center', ha='center')\n",
    "\n",
    "# Plot our steps\n",
    "ax1.plot(x_gd1, y_gd1, 'ro')\n",
    "for i in range(1, epoch+1):\n",
    "    ax1.annotate('', xy=(x_gd1[i], y_gd1[i]), xytext=(x_gd1[i-1], y_gd1[i-1]),\n",
    "                   arrowprops={'arrowstyle': '->', 'color': 'r', 'lw': 1},\n",
    "                   va='center', ha='center')\n",
    "    \n",
    "# Plot our steps\n",
    "ax1.plot(x_gd5, y_gd5, 'go')\n",
    "for i in range(1, epoch+1):\n",
    "    ax1.annotate('', xy=(x_gd5[i], y_gd5[i]), xytext=(x_gd5[i-1], y_gd5[i-1]),\n",
    "                   arrowprops={'arrowstyle': '->', 'color': 'g', 'lw': 1},\n",
    "                   va='center', ha='center')\n",
    "\n",
    "# Plot our steps\n",
    "ax1.plot(x_gd2, y_gd2, 'co')\n",
    "for i in range(1, epoch+1):\n",
    "    ax1.annotate('', xy=(x_gd2[i], y_gd2[i]), xytext=(x_gd2[i-1], y_gd2[i-1]),\n",
    "                   arrowprops={'arrowstyle': '->', 'color': 'c', 'lw': 1},\n",
    "                   va='center', ha='center')   \n",
    "    \n",
    "# Plot our steps\n",
    "ax1.plot(x_gd6, y_gd6, 'mo')\n",
    "for i in range(1, epoch+1):\n",
    "    ax1.annotate('', xy=(x_gd6[i], y_gd6[i]), xytext=(x_gd6[i-1], y_gd6[i-1]),\n",
    "                   arrowprops={'arrowstyle': '->', 'color': 'm', 'lw': 1},\n",
    "                   va='center', ha='center')\n",
    "    \n",
    "legend_elements = [Line2D([0], [0], color='r', lw=4, label='GD'),\n",
    "                  Line2D([0], [0], color='c', lw=4, label='MOM'),\n",
    "                  Line2D([0], [0], color='g', lw=4, label='AGrad'),\n",
    "                  Line2D([0], [0], color='m', lw=4, label='RMSProp'),\n",
    "                  Line2D([0], [0], color='k', lw=4, label='Adam')]\n",
    "ax1.legend(handles=legend_elements, loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All code together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "WhichFx = 3\n",
    "\n",
    "if(WhichFx==1):\n",
    "    def func_z(x, y):\n",
    "        z = x**2/10. + x*y/50. + y**2.\n",
    "        return z\n",
    "    def der_x(x, y):\n",
    "        return (2*x/10. + y/50.)\n",
    "    def der_y(x, y):\n",
    "        return (x/50. + 2*y/1.)\n",
    "    bounds = np.asarray([-3,3])\n",
    "elif(WhichFx==2):\n",
    "    def func_z(x, y):\n",
    "        z = x**2 + 3 * np.sin(y)\n",
    "        return z\n",
    "    def der_x(x, y):\n",
    "        return 2*x\n",
    "    def der_y(x, y):\n",
    "        return 3 * np.cos(y)    \n",
    "    bounds = np.asarray([-6,6])\n",
    "if(WhichFx==3):\n",
    "    def func_z(x, y):\n",
    "        z = x**4 - 2*x**2 + y**2\n",
    "        return z\n",
    "    def der_x(x, y):\n",
    "        return 4*x**3 - 4*x\n",
    "    def der_y(x, y):\n",
    "        return 2*y\n",
    "    bounds = np.asarray([-3,3])\n",
    "    \n",
    "if(WhichFx==1):\n",
    "    # location to start at\n",
    "    x0 = np.random.uniform(0,1) * (bounds[1]-bounds[0]) + bounds[0] # 3\n",
    "    y0 = np.random.uniform(0,1) * (bounds[1]-bounds[0]) + bounds[0] # 3\n",
    "    # learning rate\n",
    "    learning_rate = 0.1\n",
    "    # number of epochs\n",
    "    epoch = 100\n",
    "elif(WhichFx==2):\n",
    "    # location to start at\n",
    "    x0 = np.random.uniform(0,1) * (bounds[1]-bounds[0]) + bounds[0] # 3\n",
    "    y0 = np.random.uniform(0,1) * (bounds[1]-bounds[0]) + bounds[0] # 3\n",
    "    learning_rate = 0.1\n",
    "    epoch = 100\n",
    "elif(WhichFx==3):\n",
    "    # location to start at\n",
    "    x0 = np.random.uniform(0,1) * (bounds[1]-bounds[0]) + bounds[0] # 3\n",
    "    y0 = np.random.uniform(0,1) * (bounds[1]-bounds[0]) + bounds[0] # 3\n",
    "    learning_rate = 0.15\n",
    "    epoch = 100    \n",
    "    \n",
    "x_gd1, y_gd1, z_gd1 = gradient_descentGD(x0, y0, learning_rate, epoch)\n",
    "x_gd2, y_gd2, z_gd2 = gradient_descentMOM(x0, y0, learning_rate, epoch, mom=0.25)\n",
    "x_gd5, y_gd5, z_gd5 = gradient_descentAdaGrad(x0, y0, learning_rate, epoch)\n",
    "x_gd6, y_gd6, z_gd6 = gradient_descentRMSProp(x0, y0, learning_rate, epoch, decay_rate=0.99)\n",
    "x_gd7, y_gd7, z_gd7 = gradient_descentAdam(x0, y0, learning_rate, epoch, beta1=0.3, beta2=0.99)\n",
    "\n",
    "a = np.arange(bounds[0], bounds[1], 0.05)\n",
    "b = np.arange(bounds[0], bounds[1], 0.05)\n",
    "x, y = np.meshgrid(a, b)\n",
    "z = func_z(x, y)\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.contour(x, y, z, levels=np.logspace(bounds[0], bounds[1], 50), cmap='jet')\n",
    "\n",
    "# Plot our steps\n",
    "ax1.plot(x_gd7, y_gd7, 'ko')\n",
    "for i in range(1, epoch+1):\n",
    "    ax1.annotate('', xy=(x_gd7[i], y_gd7[i]), xytext=(x_gd7[i-1], y_gd7[i-1]),\n",
    "                   arrowprops={'arrowstyle': '->', 'color': 'k', 'lw': 1},\n",
    "                   va='center', ha='center')\n",
    "\n",
    "# Plot our steps\n",
    "ax1.plot(x_gd1, y_gd1, 'ro')\n",
    "for i in range(1, epoch+1):\n",
    "    ax1.annotate('', xy=(x_gd1[i], y_gd1[i]), xytext=(x_gd1[i-1], y_gd1[i-1]),\n",
    "                   arrowprops={'arrowstyle': '->', 'color': 'r', 'lw': 1},\n",
    "                   va='center', ha='center')\n",
    "    \n",
    "# Plot our steps\n",
    "ax1.plot(x_gd5, y_gd5, 'go')\n",
    "for i in range(1, epoch+1):\n",
    "    ax1.annotate('', xy=(x_gd5[i], y_gd5[i]), xytext=(x_gd5[i-1], y_gd5[i-1]),\n",
    "                   arrowprops={'arrowstyle': '->', 'color': 'g', 'lw': 1},\n",
    "                   va='center', ha='center')\n",
    "\n",
    "# Plot our steps\n",
    "ax1.plot(x_gd2, y_gd2, 'co')\n",
    "for i in range(1, epoch+1):\n",
    "    ax1.annotate('', xy=(x_gd2[i], y_gd2[i]), xytext=(x_gd2[i-1], y_gd2[i-1]),\n",
    "                   arrowprops={'arrowstyle': '->', 'color': 'c', 'lw': 1},\n",
    "                   va='center', ha='center')   \n",
    "    \n",
    "# Plot our steps\n",
    "ax1.plot(x_gd6, y_gd6, 'mo')\n",
    "for i in range(1, epoch+1):\n",
    "    ax1.annotate('', xy=(x_gd6[i], y_gd6[i]), xytext=(x_gd6[i-1], y_gd6[i-1]),\n",
    "                   arrowprops={'arrowstyle': '->', 'color': 'm', 'lw': 1},\n",
    "                   va='center', ha='center')\n",
    "    \n",
    "legend_elements = [Line2D([0], [0], color='r', lw=4, label='GD'),\n",
    "                  Line2D([0], [0], color='c', lw=4, label='MOM'),\n",
    "                  Line2D([0], [0], color='g', lw=4, label='AGrad'),\n",
    "                  Line2D([0], [0], color='m', lw=4, label='RMSProp'),\n",
    "                  Line2D([0], [0], color='k', lw=4, label='Adam')]\n",
    "ax1.legend(handles=legend_elements, loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
