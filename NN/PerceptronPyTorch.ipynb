{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron in PyTorch\n",
    "\n",
    "The goal of this Jupyter Notebook page is to code up the perceptron in PyTorch\n",
    "\n",
    "There are MANY ways we can achieve this, the following is just one solution. More on the web if you care, e.g.,\n",
    "\n",
    "  * https://medium.com/@tomgrek/building-your-first-neural-net-from-scratch-with-pytorch-56b0e9c84d54\n",
    "  * https://pytorch.org/docs/stable/nn.html\n",
    "  \n",
    "If you already know PyTorch, maybe your kung fu is greater than mine... I tried to keep this readable also (vs linear algebra-ized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # this is our pytorch lib\n",
    "from torch.autograd import Variable # need this for variables and autograd is for automatically working with gradients (its really nice)\n",
    "import torch.nn as nn # this is the neural net part of torch\n",
    "import torch.nn.functional as F # will use this below for some nn function stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets define a simple neuron class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(nn.Module): # I named the Python class Perceptron\n",
    "    def __init__(self): # initilization function (all classes have a __init__ you can override)\n",
    "        super(Perceptron, self).__init__() # calls the super function (from nn.Module)\n",
    "                                           # self is this \"instance\"\n",
    "        self.fc1 = nn.Linear(1,1,True) # https://pytorch-zh.readthedocs.io/en/latest/nn.html\n",
    "                                  # applies a linear transformation to the incoming data (aka our dot product!)\n",
    "                                  # nn.Linear(num of inputs, num of outputs, bias = True or False)\n",
    "                                  # holds internal Tensors for its weight and bias\n",
    "                                  # this primes the pump for things to come (i.e., we will call it in forward fx)\n",
    "    def forward(self, x): # this is the forward neural net pass\n",
    "        x = self.fc1(x) # take input x and call \"fc1\" (from __init__), which holds onto our linear equation\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that was simple!!!!\n",
    "\n",
    "Few quick remarks\n",
    "\n",
    "  * Keep all code in *PyTorch talk* (we will talk about this more later), meaning all functions are *PyTorch supported*, and PyTorch will calc the gradient (so we can train our net!) for \"free\"! (well, its not magic, it gets it from autograd)\n",
    "    * i.e., you do NOT need to define the backward pass with hand calc derivatives \n",
    "  * Neat, right!!! ...\n",
    "    * scary for your Prof, who wants to make sure you understand every calc and do all this outside PyTorch... \n",
    "\n",
    "If you want to dig deeper\n",
    "\n",
    "  * Read https://pytorch.org/docs/stable/nn.html\n",
    "    * CLASS torch.nn.Linear(in_features, out_features, bias=True)\n",
    "  * Read https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html \n",
    "    * Custom gradients\n",
    "      * https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html\n",
    "      * https://jhui.github.io/2018/02/09/PyTorch-Variables-functionals-and-Autograd/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at what we got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of our class\n",
    "net = Perceptron()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, shows the variables and their values\n",
    "\n",
    "Next, lets see our parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(net.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FYI, nice helper site: https://pytorch.org/docs/stable/torch.html\n",
    "\n",
    "Next, lets create a variable, we want to pass data into this net.\n",
    "\n",
    "A Variable wraps a Tensor. It supports nearly all the API’s defined by a Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = Variable(torch.randn(1,1,1), requires_grad=False)\n",
    "print(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting requires_grad means it’s an optimizable variable (e.g., w.r.t. autograd). We don't need that here on our input. \n",
    "\n",
    "Next, lets put the input into our neuron and see what we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that makes sense if you think about it. (0.7544 * -0.1329 ) + -0.3152 = -0.4154\n",
    "\n",
    "Of course that was for the random numbers I got. You would have to use what your Jupyter execution of that code gave to verify the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, define a loss function and an optimizer using stochastic gradient descent (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# defining our own custom function: sum of squared error (SSE)\n",
    "# you can also just call their built in ones\n",
    "def criterion(out, label):\n",
    "    return (label - out)**2\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, define a training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(1,3), (2,6), (3,9), (4,12), (5,15), (6,18)]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets do some training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEpochs = 20\n",
    "for epoch in range(NEpochs): \n",
    "    for i, data2 in enumerate(data):\n",
    "        \n",
    "        # pulls out each sample (iterates over the list)\n",
    "        X, Y = iter(data2) \n",
    "        # now, we make a variable out of this stuff\n",
    "        X, Y = Variable(torch.FloatTensor([X]), requires_grad=True), Variable(torch.FloatTensor([Y]), requires_grad=False)\n",
    "        # sets gradients of all model parameters to zero\n",
    "        optimizer.zero_grad()\n",
    "        # eval data\n",
    "        outputs = net(X)\n",
    "        # calc our error\n",
    "        loss = criterion(outputs, Y)\n",
    "        # run backward pass\n",
    "        loss.backward()\n",
    "        # take a step in optimization\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(\"Epoch {} - loss: {}\".format(epoch, loss.data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What solution did we get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(net.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets predict a value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net(Variable(torch.Tensor([[[1]]]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: PyTorch Perceptron on a more complicated (ok, slightly...) data set \n",
    "\n",
    "Lets do two class data and use PyTorch for solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import Function\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# our neural net class\n",
    "class Perceptron(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Perceptron, self).__init__()\n",
    "        self.fc1 = nn.Linear(2,1,True)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "# make the neuron\n",
    "net = Perceptron()\n",
    "# net.cuda() # GPU acceleration\n",
    "print(\"net\");\n",
    "print(net)\n",
    "print(\"net parameters\")\n",
    "print(list(net.parameters()))\n",
    "\n",
    "# criteria fx\n",
    "def criterion(out,label):\n",
    "    return (label - out)**2\n",
    "\n",
    "# setup SGD optimization\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "# make some synthetic data\n",
    "N = 100\n",
    "Nhalf = int( N / 2 )\n",
    "data = torch.randn(N,2) \n",
    "for i in range(Nhalf): # class 1\n",
    "    data[i,0] = data[i,0] * 0.1 + 0.2\n",
    "    data[i,1] = data[i,1] * 0.1 + 0.2\n",
    "for i in range(Nhalf): # class 2\n",
    "    data[i+Nhalf,0] = data[i+Nhalf,0] * 0.1 + -0.2\n",
    "    data[i+Nhalf,1] = data[i+Nhalf,1] * 0.1 + 0.8\n",
    "    \n",
    "# show what that data looks like\n",
    "dataForPlot = np.asarray( data ) \n",
    "fig = plt.figure()\n",
    "plt.plot( dataForPlot[0:Nhalf-1,0], dataForPlot[0:Nhalf-1,1], 'rx' )\n",
    "plt.plot( dataForPlot[Nhalf:N,0], dataForPlot[Nhalf:N,1], 'mx' )\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "# labels\n",
    "L = torch.ones(N)\n",
    "for i in range(Nhalf): # class 1\n",
    "    L[i] = 1\n",
    "for i in range(Nhalf): # class 2\n",
    "    L[i+Nhalf] = -1\n",
    "\n",
    "# train\n",
    "for epoch in range(500):\n",
    "    for i in range(N):\n",
    "        X = Variable(data[i,:])\n",
    "        Y = Variable(L[i])\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(X)\n",
    "        loss = criterion(outputs, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "print( \"Epoch {} - loss: {}\".format(epoch, loss.data[0]))\n",
    "print(list(net.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intercepts are?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())\n",
    "print(params[0])\n",
    "w = params[0].data.cpu().numpy()[0]\n",
    "print(w)\n",
    "print(params[1].size())\n",
    "print(params[1])\n",
    "bias = params[1].data.cpu().numpy()[0]\n",
    "print(bias)\n",
    "\n",
    "print( \"x-axis = -bias / weight1 = \" + str(-bias / w[0]) )\n",
    "print( \"y-axis = -bias / weight2 = \" + str(-bias / w[1]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now plot the perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sx = ( 0 - (-bias / w[0]) )\n",
    "sy = ( (-bias / w[1]) - 0 )\n",
    "x = np.linspace(0,1,100)\n",
    "y = (sy/sx)*x + (-bias / w[1])\n",
    "\n",
    "dataForPlot = np.asarray( data ) \n",
    "fig = plt.figure()\n",
    "plt.plot( dataForPlot[0:Nhalf-1,0], dataForPlot[0:Nhalf-1,1], 'rx' )\n",
    "plt.plot( dataForPlot[Nhalf:N,0], dataForPlot[Nhalf:N,1], 'mx' )\n",
    "plt.plot( [-bias / w[0],0] , [0,-bias / w[1]] , 'k' )\n",
    "plt.plot(x, y, '-c')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets draw with PyTorch..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the decision boundary\n",
    "# Determine grid range in x and y directions\n",
    "x_min, x_max = data[:, 0].min()-0.1, data[:, 0].max()+0.1\n",
    "y_min, y_max = data[:, 1].min()-0.1, data[:, 1].max()+0.1\n",
    "\n",
    "# Set grid spacing parameter\n",
    "spacing = min(x_max - x_min, y_max - y_min) / 100\n",
    "\n",
    "# Create grid\n",
    "XX, YY = np.meshgrid(np.arange(x_min, x_max, spacing),\n",
    "               np.arange(y_min, y_max, spacing))\n",
    "\n",
    "# Concatenate data to match input\n",
    "datax = np.hstack((XX.ravel().reshape(-1,1), \n",
    "                  YY.ravel().reshape(-1,1)))\n",
    "\n",
    "# Pass data to predict method\n",
    "data_t = torch.FloatTensor(datax)\n",
    "db_prob = net(data_t)\n",
    "\n",
    "clf = np.where(db_prob<0,0,1)\n",
    "\n",
    "Z = clf.reshape(XX.shape)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.contourf(XX, YY, Z, cmap=plt.cm.Accent, alpha=0.5)\n",
    "plt.scatter(data[:,0], data[:,1], c=L, \n",
    "            cmap=plt.cm.Accent)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection\n",
    "\n",
    "Address the following\n",
    "\n",
    "1) Add a nonlinearity/activation function\n",
    "\n",
    "2) Use a different optimization algorithm (not SGD) and use a different error/cost function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
