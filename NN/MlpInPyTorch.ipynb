{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Layer Perceptron (MLP)\n",
    "\n",
    "<img src=\"Images/Figure 3.1.jpg\" width=\"80%\">\n",
    "\n",
    "Here are some additional resources (beyond what we cover in class)\n",
    "\n",
    "* Gradient descent, how neural networks learn\n",
    "  * https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=11s&index=3\n",
    "\n",
    "* Backpropagation calculus\n",
    "  * https://www.youtube.com/watch?v=tIeHLnjs5U8&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=389s&index=5\n",
    "\n",
    "* What is backpropagation really doing\n",
    "  * https://www.youtube.com/watch?v=Ilg3gGewQ5U&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=532s&index=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modern 'language'(s) for implementing a MLP \n",
    "\n",
    "There are many web resources nowadays for learning languages like PyTorch and TensorFlow, e.g.,\n",
    "\n",
    "  * https://adventuresinmachinelearning.com/pytorch-tutorial-deep-learning/\n",
    "\n",
    "On this page, we are going to create a simple MLP in PyTorch.\n",
    "\n",
    "Specifically, we will look at the XOR problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include our Python packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import Function\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "##############################################\n",
    "# declare the MLP\n",
    "##############################################\n",
    "\n",
    "# lets make a simple MLP\n",
    "class XORMlp(nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(XORMlp, self).__init__()\n",
    "        self.linear1 = nn.Linear(D_in, H) # input to hidden layer\n",
    "        self.linear2 = nn.Linear(H, D_out) # hidden layer to output\n",
    "    def forward(self, x):\n",
    "        h_pred = F.relu(self.linear1(x)) # h = dot(input,w1) \n",
    "                                         #  and nonlinearity (relu)\n",
    "        y_pred = self.linear2(h_pred) # network_output = dot(h,w2)\n",
    "        return y_pred\n",
    "\n",
    "##############################################\n",
    "# create an instance and set up optimization\n",
    "##############################################\n",
    "\n",
    "# here is a network with 2 inputs to 4 hidden neurons to one output neuron    \n",
    "D_in, H, D_out = 2, 4, 1    \n",
    "net = XORMlp(D_in, H, D_out)\n",
    "\n",
    "# now, optimization and draw stuff (look at perceptron Jupyter pages)\n",
    "\n",
    "def criterion(out,label):\n",
    "    return (label - out)**2\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.3)\n",
    "\n",
    "##############################################\n",
    "# xor data set\n",
    "##############################################\n",
    "\n",
    "data = torch.randn(4,2)\n",
    "data[0,0] = 0; data[0,1] = 0;\n",
    "data[1,0] = 1; data[1,1] = 1;\n",
    "data[2,0] = 0; data[2,1] = 1;\n",
    "data[3,0] = 1; data[3,1] = 0;\n",
    "\n",
    "L = torch.randn(4)\n",
    "L[0] = 0; L[1] = 0\n",
    "L[2] = 1; L[3] = 1\n",
    "\n",
    "##############################################\n",
    "# training\n",
    "##############################################\n",
    "\n",
    "for epoch in range(1500):\n",
    "    for i in range(4):\n",
    "        X = Variable(data[i,:])\n",
    "        Y = Variable(L[i])\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(X)\n",
    "        loss = criterion(outputs, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print(net(Variable(torch.Tensor([[[0,0]]]))))\n",
    "print(net(Variable(torch.Tensor([[[1,0]]]))))\n",
    "print(net(Variable(torch.Tensor([[[0,1]]]))))\n",
    "print(net(Variable(torch.Tensor([[[1,1]]]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, lets draw it using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot decision boundary\n",
    "x_min, x_max = data[:, 0].min()-0.1, data[:, 0].max()+0.1\n",
    "y_min, y_max = data[:, 1].min()-0.1, data[:, 1].max()+0.1\n",
    "spacing = min(x_max - x_min, y_max - y_min) / 100\n",
    "XX, YY = np.meshgrid(np.arange(x_min, x_max, spacing),\n",
    "               np.arange(y_min, y_max, spacing))\n",
    "datax = np.hstack((XX.ravel().reshape(-1,1), \n",
    "                  YY.ravel().reshape(-1,1)))\n",
    "data_t = torch.FloatTensor(datax)\n",
    "db_prob = net(data_t)\n",
    "clf = np.where(db_prob<0.5,0,1)\n",
    "Z = clf.reshape(XX.shape)\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.contourf(XX, YY, Z, cmap=plt.cm.Accent, alpha=0.5)\n",
    "plt.scatter(data[:,0], data[:,1], c=L, cmap=plt.cm.Accent)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2: Another Way to Write It, Yup\n",
    "\n",
    "This page simply looks at PyTorch's nn.Sequential container!\n",
    "  * https://pytorch.org/docs/stable/nn.html\n",
    "  * https://www.programcreek.com/python/example/107650/torch.nn.Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import Function\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "############################################\n",
    "# Here is what's different in this code!!!\n",
    "#   self.layers = nn.Sequential\n",
    "############################################\n",
    "class AnotherMLP(nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(AnotherMLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(D_in, H),\n",
    "            nn.Tanh(),  # nn.ReLU()\n",
    "            nn.Linear(H, D_out)\n",
    "        )        \n",
    "    def forward(self, x):\n",
    "        y_pred = self.layers(x)\n",
    "        return y_pred\n",
    "############################################\n",
    "\n",
    "D_in, H, D_out = 2, 2, 1    \n",
    "net = AnotherMLP(D_in, H, D_out)\n",
    "\n",
    "def criterion(out,label):\n",
    "    return (label - out)**2\n",
    "\n",
    "############################################\n",
    "# threw in a new learner for you!\n",
    "############################################\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-1)\n",
    "\n",
    "# xor data set\n",
    "data = torch.randn(4,2)\n",
    "data[0,0] = 0; data[0,1] = 0;\n",
    "data[1,0] = 1; data[1,1] = 1;\n",
    "data[2,0] = 0; data[2,1] = 1;\n",
    "data[3,0] = 1; data[3,1] = 0;\n",
    "\n",
    "L = torch.randn(4)\n",
    "L[0] = 0; L[1] = 0\n",
    "L[2] = 1; L[3] = 1\n",
    "\n",
    "for epoch in range(2000):\n",
    "    for i in range(4):\n",
    "        X = Variable(data[i,:])\n",
    "        Y = Variable(L[i])\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(X)\n",
    "        loss = criterion(outputs, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "x_min, x_max = data[:, 0].min()-0.1, data[:, 0].max()+0.1\n",
    "y_min, y_max = data[:, 1].min()-0.1, data[:, 1].max()+0.1\n",
    "spacing = min(x_max - x_min, y_max - y_min) / 100\n",
    "XX, YY = np.meshgrid(np.arange(x_min, x_max, spacing),\n",
    "               np.arange(y_min, y_max, spacing))\n",
    "datax = np.hstack((XX.ravel().reshape(-1,1), \n",
    "                  YY.ravel().reshape(-1,1)))\n",
    "data_t = torch.FloatTensor(datax)\n",
    "db_prob = net(data_t)\n",
    "clf = np.where(db_prob<0.5,0,1)\n",
    "Z = clf.reshape(XX.shape)\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.contourf(XX, YY, Z, cmap=plt.cm.Accent, alpha=0.5)\n",
    "plt.scatter(data[:,0], data[:,1], c=L, cmap=plt.cm.Accent)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection\n",
    "\n",
    "1) Change the nonlinearity\n",
    "\n",
    "2) Try different #s of layers and neurons\n",
    "\n",
    "3) Make a different data set\n",
    "\n",
    "4) Does it matter what labels you pick? e.g., {0,1} or {-1,1}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
