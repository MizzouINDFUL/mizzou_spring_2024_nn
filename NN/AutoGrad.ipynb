{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Graphs (CG), Calculus on CG, and AutoGrad\n",
    "\n",
    "Useful links\n",
    "  * https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\n",
    "  * https://colah.github.io/posts/2015-08-Backprop/\n",
    "  * https://www.deepideas.net/deep-learning-from-scratch-i-computational-graphs/\n",
    "  \n",
    "What is torch.autograd?\n",
    " * PyTorch's automatic differentiation engine\n",
    " * Will help you train your neural networks!\n",
    " * Breakdown\n",
    "   * forward prop: pushes input data through the net\n",
    "   * backwar prop: what we have been studying! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create two tensors with autograd turned on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create another tensor based on a and b\n",
    "\n",
    "$Q = 3a^3 - b^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 3*a**3 - b**2\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we calc'd the gradients of Q we will get (analytically)\n",
    "\n",
    "$\\frac{\\partial Q}{\\partial a} = 9a^2$\n",
    "\n",
    "$\\frac{\\partial Q}{\\partial b} = -2b$\n",
    "\n",
    "We can call .backward() on our tensor $Q$ \n",
    "\n",
    "Autograd calc's the gradients and stores them in the respective tensor's .grad attribute\n",
    "\n",
    "However, we need to explicitly pass a gradient argument in Q.backward() because its a vector\n",
    "\n",
    "Gradient is a tensor of the same shape as Q and it represents the gradient of Q w.r.t. itself\n",
    "\n",
    "$\\frac{dQ}{dQ}=1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_grad = torch.tensor([1., 1.])\n",
    "Q.backward(gradient=external_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have those gradients sitting for us in a and b!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a.grad)\n",
    "print(9*a**2)\n",
    "\n",
    "print(b.grad)\n",
    "print(-2*b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, right!\n",
    "\n",
    "Let's do an example by hand vs all that PyTorch magic, lets do the Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# define a data set\n",
    "X = np.array([\n",
    "    [0, 1],  # data point @ (0,1)\n",
    "    [1, 1],  # data point @ (1,1)\n",
    "    [0, -1],  # data point @ (0,-1)\n",
    "    [1, -1],  # data point @ (1,-1)\n",
    "]) \n",
    "\n",
    "# DESIRED labels\n",
    "y = np.array([[-1], \n",
    "              [-1], \n",
    "              [1],\n",
    "              [1]\n",
    "             ])\n",
    "\n",
    "# initial weight vector\n",
    "w = np.asarray([1.,-0.5])\n",
    "\n",
    "# push our wrong data point through the network! (other points are at least on the right size of the 0 check!)\n",
    "v = np.sum(X[1,:] * w)\n",
    "y = (np.exp(v)-np.exp(-v)) / (np.exp(v)+np.exp(-v))\n",
    "print( y )\n",
    "print( 'wanted it to be negative and a -1' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets do its update manually with tensors and autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our forward pass\n",
    "\n",
    "# input and weight vector\n",
    "w = torch.tensor([1., -0.5], requires_grad=True)\n",
    "x = torch.tensor([1., 1.], requires_grad=True)\n",
    "\n",
    "# our network formula\n",
    "v = torch.dot(x,w)\n",
    "v.retain_grad()\n",
    "y = (torch.exp(v)-torch.exp(-v)) / (torch.exp(v)+torch.exp(-v))\n",
    "y.retain_grad()\n",
    "e = 0.5 * ( -1. - y )**2\n",
    "\n",
    "print(y)\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, I put in .retain_grad() to keep non-leaf node gradients (those intermediate variables!)\n",
    "\n",
    "Lets evaluate that backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.backward()\n",
    "print('y grad')\n",
    "print(y.grad)\n",
    "print('v grad')\n",
    "print(v.grad)\n",
    "print('w grad')\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets work out the gradients by hand that we would have done (see our formula from the backprop day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('y grad')\n",
    "print( (-1 - y) * (-1) )\n",
    "print('v grad')\n",
    "print( (-1 - y) * (-1) * (1 - torch.tanh(v)**2) )\n",
    "print('w grad')\n",
    "print( [ (-1 - y) * (-1) * (1 - torch.tanh(v)**2) * (1.), (-1 - y) * (-1) * (1 - torch.tanh(v)**2) * (1.) ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can take it from here class!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
