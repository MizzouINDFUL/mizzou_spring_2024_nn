{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.linalg import lstsq\n",
    "import sys\n",
    "import argparse\n",
    "from torchvision import datasets\n",
    "import torch.nn as nn\n",
    "\n",
    "##############################################\n",
    "# this is our class (look in local directory) for making a PyTorch \"dataset\" for minibatch\n",
    "##############################################\n",
    "\n",
    "from dataset_utils import Format_Dataset\n",
    "\n",
    "##############################################\n",
    "# lets define our neural network\n",
    "##############################################\n",
    "\n",
    "class MyMLP(torch.nn.Module):\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_pred = self.layers(x)\n",
    "        return y_pred\n",
    "\n",
    "    def __init__(self, D_in, H1, H2, D_out):\n",
    "        super(MyMLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(D_in, H1),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(H1,H2),\n",
    "            nn.Sigmoid(),            \n",
    "            nn.Linear(H2, D_out),\n",
    "            nn.Sigmoid()\n",
    "        )  \n",
    "        \n",
    "##############################################\n",
    "# lets use a random seed so we can get deterministic results run-2-run\n",
    "##############################################\n",
    "\n",
    "torch.manual_seed(3527)\n",
    "\n",
    "##############################################\n",
    "# dataset\n",
    "##############################################\n",
    "\n",
    "# data -----------------------------------\n",
    "NoSamples = 400\n",
    "NoPatterns = 2 \n",
    "x = torch.zeros(NoSamples,2)\n",
    "\n",
    "# labels ---------------------------------\n",
    "l = torch.zeros(NoSamples,2)\n",
    "l[0:int(NoSamples/2),0] = 1.0\n",
    "l[int(NoSamples/2):,1] = 1.0\n",
    "\n",
    "# randomly pick our data\n",
    "Membs = torch.rand(NoPatterns,2) # the means \n",
    "Stds = torch.rand(NoPatterns,2)*.1 # the standard deviations \n",
    "dc = 0\n",
    "for r in range(NoPatterns):\n",
    "    tm = torch.Tensor([Membs[r,0], Membs[r,1]]) \n",
    "    tc = torch.eye(2)\n",
    "    tc[0,0] = Stds[r,0]*Stds[r,0]\n",
    "    tc[1,1] = Stds[r,1]*Stds[r,1]\n",
    "    m = torch.distributions.multivariate_normal.MultivariateNormal(tm,tc)\n",
    "    for i in range(int(NoSamples/NoPatterns)):\n",
    "        x[dc,:] = m.sample()\n",
    "        dc = dc + 1\n",
    "\n",
    "##############################################\n",
    "# convert this into our PyTorch dataset object\n",
    "##############################################\n",
    "\n",
    "# make up training data set\n",
    "dataset = {'samples': x, 'labels': l} \n",
    "train = Format_Dataset(dataset, choice = 'Train')\n",
    "train.samples = train.samples.to(torch.float32)\n",
    "train.labels = train.labels.to(torch.float32)\n",
    "train = torch.utils.data.DataLoader( shuffle = False,\n",
    "                                        dataset = train, \n",
    "                                        batch_size = 30 ) # here is our mini-batch size\n",
    "\n",
    "##############################################\n",
    "# create a network and set up optimization\n",
    "##############################################\n",
    "\n",
    "D_in, H1, H2, D_out = 2, 16, 16, 2                                              \n",
    "net = MyMLP(D_in, H1, H2, D_out)\n",
    "\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-2)\n",
    "\n",
    "##############################################\n",
    "# learning\n",
    "##############################################\n",
    "\n",
    "NoEpochs = 500\n",
    "for epoch in tqdm(range( NoEpochs ),'Training Epoch'):\n",
    "    for sample, label in train:\n",
    "        outputs = net(sample)             # this passes in a set of data\n",
    "        loss = criterion(outputs, label)  # forward pass on set of data\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()            \n",
    "print('Done learning')\n",
    "\n",
    "##############################################\n",
    "# draw\n",
    "##############################################\n",
    "\n",
    "x_min, x_max = x[:, 0].min()-0.1, x[:, 0].max()+0.1\n",
    "y_min, y_max = x[:, 1].min()-0.1, x[:, 1].max()+0.1\n",
    "spacing = min(x_max - x_min, y_max - y_min) / 100\n",
    "XX, YY = np.meshgrid(np.arange(x_min, x_max, spacing),\n",
    "            np.arange(y_min, y_max, spacing))\n",
    "datax = np.hstack((XX.ravel().reshape(-1,1), \n",
    "                YY.ravel().reshape(-1,1)))\n",
    "data_t = torch.FloatTensor(datax)\n",
    "db_prob = net(data_t)\n",
    "db_vals, clf = torch.max(db_prob, 1)\n",
    "Z = clf.reshape(XX.shape)\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.contourf(XX, YY, Z, cmap=plt.cm.Accent, alpha=0.5)\n",
    "L = np.zeros(NoSamples)\n",
    "L[0:int(NoSamples/2)] = 1.0\n",
    "plt.scatter(x[:,0], x[:,1], c=L, cmap=plt.cm.Accent)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
