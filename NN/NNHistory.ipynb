{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://beamandrew.github.io//images/deep_learning_101/nn_timeline.jpg\" width=\"100%\">\n",
    "<img src=\"https://miro.medium.com/max/786/1*vuW6DmvB7PeNM8vddesC3Q.webp\" width=\"100%\">\n",
    "\n",
    "# BRIEF History of SOME neural networks :)\n",
    "\n",
    "**Disclaimers:**\n",
    "\n",
    "  * There are MANY papers and authors not listed below\n",
    "    * Many specifically with respect to our recent flurry (flood?) of NNs\n",
    "  * There is often debate between who really did what, when, and first :-)\n",
    "  * Its nearly impossible nowadays to keep up with NN publications (1,000s, 10,000s, more?)\n",
    "  * The point is to get you to appreciate the HISTORY, up's and down's, key players, etc. \n",
    "\n",
    "* A few additional resources if you want to dig deeper\n",
    "\n",
    "  * https://towardsdatascience.com/a-concise-history-of-neural-networks-2070655d3fec\n",
    "  * https://cs.stanford.edu/people/eroberts/courses/soco/projects/neural-networks/History/history1.html\n",
    "  * https://cs.stanford.edu/people/eroberts/courses/soco/projects/neural-networks/History/history2.html\n",
    "  * https://en.wikipedia.org/wiki/Perceptrons_(book)\n",
    "  * http://people.idsia.ch/~juergen/who-invented-backpropagation.html\n",
    "  * https://en.wikipedia.org/wiki/Paul_Werbos\n",
    "  * https://ieeexplore.ieee.org/document/623220\n",
    "  * https://beamandrew.github.io/deeplearning/2017/02/23/deep_learning_101_part1.html\n",
    "  * https://www.import.io/post/history-of-deep-learning/\n",
    "  * https://medium.com/@sidereal/cnns-architectures-lenet-alexnet-vgg-googlenet-resnet-and-more-666091488df5\n",
    "  \n",
    "## Highlights\n",
    "\n",
    "  * 1943: neurophysiologist Warren McCulloch and mathematician Walter Pitts wrote a paper on how neurons might work. In order to describe how neurons in the brain might work, they modeled a simple neural network using electrical circuits.\n",
    "  * 1949: Donald Hebb wrote The Organization of Behavior, a work which pointed out the fact that neural pathways are strengthened each time they are used, a concept fundamentally essential to the ways in which humans learn. If two nerves fire at the same time, he argued, the connection between them is enhanced.\n",
    "  * 1950's: it was finally possible to simulate a hypothetical neural network. The first step towards this was made by Nathanial Rochester from the IBM research laboratories. Unfortunately for him, the first attempt to do so came up short.\n",
    "  * 1959: Bernard Widrow and Marcian Hoff of Stanford developed models called \"ADALINE\" and \"MADALINE\" (Multiple ADAptive LINear Elements).\n",
    "  * 1960: Frank Rosenblatt and Perceptron\n",
    "  * 1962: Widrow & Hoff developed a learning procedure that examines the value before the weight adjusts it (i.e. 0 or 1) according to the rule.\n",
    "  * 1960s: a paper was written that suggested there could not be an extension from the single layered neural network to a multiple layered neural network. In addition, many people in the field were using a learning function that was fundamentally flawed because it was not differentiable across the entire line. As a result, research and funding went drastically down.\n",
    "  * 1969: Perceptron book written by Marvin Minsky and Seymour Papert. (note, I believe Rosenblatt and Minsky knew each other since adolescence, having studied with a one-year difference at the Bronx High School of Science). The meat of Perceptrons is a number of mathematical proofs which acknowledge some of the perceptrons' strengths while also showing major limitations. For example, XOR.\n",
    "  * 1972: Kohonen and Anderson developed a similar network independently of one another.\n",
    "  * 1974: Paul Werbos described the process of training artificial neural networks through backpropagation of errors in his PhD thesis. It did not take off per se (for whatever reasons?). \n",
    "  * 1975: The first multilayered network was developed, an unsupervised network\n",
    "  * 1979: Kunihiko Fukushima and Neocognitron. The first true convolutional neural network applied to vision.\n",
    "  * 1982: Hopfield Networks. Interest in the field was renewed. John Hopfield of Caltech presented a paper to the National Academy of Sciences. His approach was to create more useful machines by using bidirectional lines. Previously, the connections between neurons was only one way.\n",
    "  * 1986: With multiple layered neural networks in the news, the problem was how to extend the Widrow-Hoff rule to multiple layers. Three independent groups of researchers, one of which included David Rumelhart, a former member of Stanford's psychology department, came up with similar ideas which are now called back propagation networks because it distributes pattern recognition errors throughout the network.\n",
    "  * 1989: Yann LeCun and LeNet-5. Combined convolutional neural networks with recent backpropagation theories to read handwritten digits.\n",
    "  * 1989: Christopher Watkins. Watkins published his PhD thesis – “Learning from Delayed Rewards” – where he introduced the concept of Q-learning (reinforcement learning).\n",
    "  * 1993: Jürgen Schmidhuber. German computer scientist Schmidhuber solved a “very deep learning” task in 1993 that required more than 1,000 layers in the recurrent neural network.\n",
    "  * 1995: Corinna Cortes and Vladimir Vapnik. The current standard model was designed by Cortes and Vapnik in 1993 and presented in 1995.\n",
    "  * 1997: Won, Gader, Keller, and others created the morphological shared weight neural nets (MSNN). \n",
    "  * 1997: Long short-term memory. Jürgen Schmidhuber and Sepp Hochreiter. A recurrent neural network framework, long short-term memory (LSTM) was proposed by Schmidhuber and Hochreiter in 1997.\n",
    "  * 2009: Launch of ImageNet.\n",
    "  * 2012: Alex Krizhevsky and Hinton. Between 2011 and 2012, Alex Krizhevsky won several international machine and deep learning competitions with his creation AlexNet, a convolutional neural network.\n",
    "  * 2013: the ILSVRC 2013 winner was also a CNN which became known as ZFNet. It achieved a top-5 error rate of 14.8% which is now already half of the prior mentioned non-neural error rate.\n",
    "  * 2014: GoogleNet/Inception.\n",
    "  * 2014: VGGNet. \n",
    "  * 2014: Generative Adversarial Networks, Ian Goodfellow. \n",
    "  * 2015: ResNet.\n",
    "  * ...\n",
    "  \n",
    "Honestly, I am tired of typing, sorry. Not even sure what we list right now in *modern days*: e.g., U-Nets, Dilated Convolutions, Fractionally Strided Convolutions, Deconvolutional Neural Nets, Semantic Nets, NASNet, Graph Nets, Long Short Term Memory, Transformers, ... Heck, at Mizzou alone we have explored linear order statistic neurons (LOSN), Choquet integral neurons (ChiMP), shallow and deep morphological hit-or-miss transform networks, fuzzy inference networks (MLP-based, ANFIS), aggregation networks, multiple instance learning NNs, ... The point is, the above gets you started... Maybe I will try to do a *recent works* (last 5 years), which is beyond the scope of this introduction Jupyter page anyways.\n",
    "\n",
    "**My Take on Big Events Relative to CNNs and Deep Learning**\n",
    "\n",
    " * 1943: McCulloch and Pitts : basic neuron\n",
    " * 1960: Frank Rosenblatt : the Peceptron\n",
    " * 1974: Paul Werbos : backprop, aka method to train our networks\n",
    " * 1975: The first multilayered network : ability to reach interesting functions and eventually go deep!\n",
    " * 1979: Kunihiko Fukushima and Neocognitron : first \"true\" CNN (?)\n",
    " * 1982: Hopfield Networks.\n",
    " * 1989: Yann LeCun (and others like Paul Gader) : emphasis on shared weights and sparse connectivity\n",
    " * 2009, Fei-Fei Li : AI professor at Stanford launched ImageNet, assembled a free database of more than 14 million labeled images, i.e., data data data\n",
    " * 2012: Krizhevsky and Hinton and others, AlexNet : now we are talking learning algorithms and specific networks and hardware and data to train things folks!\n",
    " * note, there are others, I am stopping, I wanted to really provide some history up to 2012\n",
    " * also, I need to look up one of the biggest co-contributors, computing, e.g., GPUs!!! In 2009 I already referenced data. The combination of data and computing hardware have been BIG contributors.\n",
    " * yes, I have missed many, apologies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
