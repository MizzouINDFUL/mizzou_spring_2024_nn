{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Backprop in NumPy\n",
    "\n",
    "Below, lets consider the following \n",
    "\n",
    "* XOR data set: input is $\\textbf{x}_k = (x_1, x_2)^t$ and labels are in $y_k \\in \\{0,1\\}$.\n",
    "\n",
    "* Lets do one hidden layer, two neurons, bias terms, and Sigmoid function. Why? I said so!\n",
    "\n",
    "* One node in the output layer, bias term, and the Sigmoid function\n",
    "\n",
    "So, the forward pass looks like\n",
    "\n",
    "* $y_1 = S(\\textbf{x}^t \\textbf{w}_1 + b_1)$\n",
    "\n",
    "* $y_2 = S(\\textbf{x}^t \\textbf{w}_2 + b_2)$\n",
    "\n",
    "* $y_3 = S((y_1,y_2)^t \\textbf{w}_3 + b_3)$\n",
    "\n",
    "* where $S$ is the Sigmoid\n",
    "\n",
    "The following NumPy code intended to be readable, not optimized. We will talk in class about how to *linearalgebratize* this.\n",
    "\n",
    "First, lets declare our Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libs\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from IPython.display import clear_output\n",
    "import sys\n",
    "import random\n",
    "\n",
    "# our nonlinear Sigmoid function (including its derivative)\n",
    "#  note: I let a=1 in the Sigmoid\n",
    "def sigmoid(x, derive=False): # x is the input, derive is do derivative or not\n",
    "    if derive: # ok, says calc the deriv?\n",
    "        return x * (1.0 - x) # note, you might be thinking ( sigmoid(x) * (1 - sigmoid(x)) )\n",
    "                           # depends on how you call the function\n",
    "    return ( 1.0 / (1.0 + np.exp(-x)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then make our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the XOR data set\n",
    "X = np.array([\n",
    "    [1, 1, 1],  # data point (x,y) = (1,1), with homogenization (i.e., 1 for bias)\n",
    "    [1, 0, 1],  # data point (1,0)\n",
    "    [0, 1, 1],  # data point (0,1)\n",
    "    [0, 0, 1],  # data point (0,0)\n",
    "]) \n",
    "\n",
    "# labels\n",
    "y = np.array([[0], \n",
    "              [1], \n",
    "              [1],\n",
    "              [0]\n",
    "             ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights with random numbers\n",
    "n1_w = np.random.normal(0,2,(3, 1))\n",
    "n2_w = np.random.normal(0,2,(3, 1))\n",
    "n3_w = np.random.normal(0,2,(3, 1))\n",
    "\n",
    "print('hidden layer 1, neuron 1 weights')\n",
    "print(n1_w)\n",
    "print('hidden layer 1, neuron 2 weights')\n",
    "print(n2_w)\n",
    "print('hidden layer 2, neuron 1 weights')\n",
    "print(n3_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "# Epochs\n",
    "###############################################\n",
    "eta = 2.0 # learning rate\n",
    "err_break = 0.001 # stop when below this error\n",
    "max_epoch = 5000 # how many epochs? (each epoch will run through all 4 data points)\n",
    "err = np.zeros((max_epoch,1)) # lets record error to plot (get a convergence plot)\n",
    "end_index = max_epoch-1 # what index did we stop on?\n",
    "inds = np.asarray([0,1,2,3]) # array of our 4 indices (data point index references)\n",
    "for k in range(max_epoch): \n",
    "    \n",
    "    # init error\n",
    "    err[k] = 0    \n",
    "    \n",
    "    # random shuffle of data each epoch?\n",
    "    #inds = np.random.permutation(inds)\n",
    "    \n",
    "    # doing online, go through each point, one at a time\n",
    "    for i in range(4): \n",
    "        \n",
    "        # what index?\n",
    "        inx = inds[i]\n",
    "        \n",
    "        # forward pass\n",
    "        # layer 1\n",
    "        v = np.ones((3, 1))\n",
    "        v[0] = np.dot(X[inx,:], n1_w) # neuron 1 fires (x as input)\n",
    "        v[0] = sigmoid(v[0])        # neuron 1 sigmoid\n",
    "        v[1] = np.dot(X[inx,:], n2_w) # neuron 2 fires (x as input)\n",
    "        v[1] = sigmoid(v[1])    \n",
    "        # layer 2\n",
    "        oo = np.dot(np.transpose(v), n3_w) # neuron 3 fires, taking neuron 1 and 2 as input\n",
    "        o = sigmoid(oo) # hey, result of our net!!!\n",
    "        \n",
    "        # error\n",
    "        err[k] = err[k] + ((1.0/2.0) * np.power((y[inx] - o), 2.0))\n",
    "                \n",
    "        # backprop time folks!!!\n",
    "        \n",
    "        # output layer, our delta is (delta_1 * delta_2)\n",
    "        delta_1 = (-1.0) * (y[inx] - o)\n",
    "        delta_2 = sigmoid(o,derive=True) # note how I called it, I passed o=sigmoid(oo)\n",
    "        \n",
    "        # now, lets prop it back to the weights\n",
    "        delta_ow = np.ones((3, 1))\n",
    "        # format is\n",
    "        #  delta_index = (input to final neuron) * (Err derivative * Sigmoid derivative)\n",
    "        delta_ow[0] = v[0]  *  (delta_1*delta_2)\n",
    "        delta_ow[1] = v[1]  *  (delta_1*delta_2)\n",
    "        delta_ow[2] = v[2]  *  (delta_1*delta_2)\n",
    "        \n",
    "        # neuron n1\n",
    "        delta_3 = sigmoid(v[0],derive=True)\n",
    "        # same, need to prop back to weights\n",
    "        delta_hw1 = np.ones((3, 1))\n",
    "        # format\n",
    "        #              input     this Sig der     error from output   weight to output neuron\n",
    "        delta_hw1[0] = X[inx,0]  *  delta_3  *  ((delta_1*delta_2)   *n3_w[0])\n",
    "        delta_hw1[1] = X[inx,1]  *  delta_3  *  ((delta_1*delta_2)   *n3_w[0])\n",
    "        delta_hw1[2] = X[inx,2]  *  delta_3  *  ((delta_1*delta_2)   *n3_w[0])     \n",
    "        \n",
    "        # neuron n2\n",
    "        delta_4 = sigmoid(v[1],derive=True)\n",
    "        # same, need to prop back to weights        \n",
    "        delta_hw2 = np.ones((3, 1))\n",
    "        delta_hw2[0] = X[inx,0]  *  delta_4  *   ((delta_1*delta_2)   *n3_w[1])\n",
    "        delta_hw2[1] = X[inx,1]  *  delta_4  *   ((delta_1*delta_2)   *n3_w[1])\n",
    "        delta_hw2[2] = X[inx,2]  *  delta_4  *   ((delta_1*delta_2)   *n3_w[1])\n",
    "        \n",
    "        # update rule\n",
    "        n1_w = n1_w - eta * delta_hw1 # neuron 1 in hidden layer 1\n",
    "        n2_w = n2_w - eta * delta_hw2 # neuron 2 in hidden layer 1\n",
    "        n3_w = n3_w - eta * delta_ow  # neuron 1 in hidden layer 2\n",
    "        \n",
    "    if( err[k] < err_break ):\n",
    "        end_index = k\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the output and convergence plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Ran ' + str(end_index) + ' iterations')\n",
    "\n",
    "# plot it        \n",
    "plt.plot(err[0:end_index])\n",
    "plt.ylabel('error')\n",
    "plt.xlabel('epochs')\n",
    "plt.show()\n",
    "        \n",
    "# what were the values (just do forward pass)  \n",
    "for i in range(4): \n",
    "    \n",
    "    # forward pass\n",
    "    v = np.ones((3, 1))\n",
    "    v[0] = np.dot(X[i,:], n1_w)\n",
    "    v[0] = sigmoid(v[0])\n",
    "    v[1] = np.dot(X[i,:], n2_w)\n",
    "    v[1] = sigmoid(v[1])    \n",
    "    oo = np.dot(np.transpose(v), n3_w)\n",
    "    o = sigmoid(oo) \n",
    "    print(str(i) + \": produced: \" + str(o) + \" wanted \" + str(y[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
